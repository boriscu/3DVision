{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM789lBCgJnafbxDbMFOTFd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Problem: Absolute camera orientation given set of relative camera pairs\n","\n","Useful video: https://www.youtube.com/watch?v=MyrVDUnaqUs\n","\n","Given an optical system of $N$ cameras with extrinsics $\\{g_1, ..., g_N | g_i \\in SE(3)\\}$, and a set of relative camera positions $\\{g_{ij} | g_{ij}\\in SE(3)\\}$ that map between coordinate frames of randomly selected pairs of cameras $(i, j)$, we search for the absolute extrinsic parameters $\\{g_1, ..., g_N\\}$ that are consistent with the relative camera motions.\n","\n","*Extrinsic parameters define the absolute position and orientation of a camera in a global or world coordinate system. They tell you where the camera is located (translation) and how it is pointed (rotation) in relation to a common reference point in that world space*\n","\n","This optimization process aims to adjust the estimated camera positions and orientations so that they align as closely as possible with their true positions. It minimizes the differences between observed relative positions of camera pairs and those calculated from these adjusted positions. The goal is to refine the estimates iteratively until they accurately reflect the real-world setup of the cameras.\n","\n","More formally: $$ g_1, ..., g_N = {\\arg \\min}_{g_1, ..., g_N} \\sum_{g_{ij}} d(g_{ij}, g_i^{-1} g_j), $$, where $d(g_i, g_j)$ is a suitable metric that compares the extrinsics of cameras $g_i$ and $g_j$.\n","\n","Visually, the problem can be described as follows. The picture below depicts the situation at the beginning of our optimization. The ground truth cameras are plotted in purple while the randomly initialized estimated cameras are plotted in orange:\n","\n","![problem start](https://github.com/facebookresearch/pytorch3d/blob/main/docs/tutorials/data/bundle_adjustment_initialization.png?raw=1)\n","\n","**Our optimization seeks to align the estimated (orange) cameras with the ground truth (purple) cameras, by minimizing the discrepancies between pairs of relative cameras.**\n","\n","![problem finish](https://github.com/facebookresearch/pytorch3d/blob/main/docs/tutorials/data/bundle_adjustment_final.png?raw=1)\n","\n","In practice, the camera extrinsics $g_{ij}$ and $g_i$ are represented using objects from the SfMPerspectiveCameras class initialized with the corresponding rotation and translation matrices R_absolute and T_absolute that define the extrinsic parameters $g = (R, T); R \\in SO(3); T \\in \\mathbb{R}^3$.\n","In order to ensure that R_absolute is a valid rotation matrix, we represent it using an exponential map (implemented with so3_exp_map) of the axis-angle representation of the rotation log_R_absolute.\n","\n","Note that the solution to this problem could only be recovered up to an unknown global rigid transformation $g_{glob} \\in SE(3)$. Thus, for simplicity, we assume knowledge of the absolute extrinsics of the first camera $g_0$. We set $g_0$ as a trivial camera $g_0 = (I, \\vec{0})$.\n","\n","Visualization of bundle adjustments: https://drive.google.com/file/d/1jxER6Gqjw3-dcNx7s7PG8DXK6xTquAXk/view?usp=sharing\n"],"metadata":{"id":"rYXcE1nuyazZ"}},{"cell_type":"markdown","source":["## Terminology\n","\n","### Special Orthogonal Group\n","\n","- SO(n): The Special Orthogonal group SO(n) is the group of n×n rotation matrices with determinant 1. These matrices represent rotations in n-dimensional space and are orthogonal, meaning their inverse is their transpose.\n","- SO(3): Specifically, SO(3) refers to the group of 3x3 rotation matrices that describe all possible rotations in 3-dimensional space. In computer vision and robotics, SO(3) is crucial for representing the orientation of objects or cameras in three dimensions.\n","\n","### Special Euclidian Group\n","- SE(n): The Special Euclidean group SE(n) refers to the set of all transformations that can be described as a rotation followed by a translation. These transformations are represented using matrices that combine rotational and translational components. For n-dimensional space, SE(n) transformations are typically represented by (n+1)×(n+1) matrices.\n","- SE(3): Specifically, SE(3) involves transformations in 3-dimensional space. An SE(3) transformation matrix is a 4x4 matrix.\n","\n","Here, R is a 3x3 matrix from SO(3) representing the rotation, and T is a 3-element column vector representing the translation. The last row is usually a fixed row [0,0,0,1], ensuring the matrix is homogeneous and suitable for operations in projective space."],"metadata":{"id":"CoHNXI0n_ngM"}},{"cell_type":"markdown","source":["# Installation and Imports"],"metadata":{"id":"wHpSfi5N69rG"}},{"cell_type":"code","source":["!pip install torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0FwDuyOcC54A","executionInfo":{"status":"ok","timestamp":1743089149961,"user_tz":-60,"elapsed":102382,"user":{"displayName":"Boris Culjak","userId":"11993007517939214783"}},"outputId":"2ce95f78-610f-4234-d9f1-38b68c476c32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"code","source":["!pip install torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cir6olJgC7bW","executionInfo":{"status":"ok","timestamp":1743089154203,"user_tz":-60,"elapsed":4236,"user":{"displayName":"Boris Culjak","userId":"11993007517939214783"}},"outputId":"14b47ab8-1b3e-4e3a-bd6d-7f83c2b73465"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_uAnX0kyO19","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743091063305,"user_tz":-60,"elapsed":1909099,"user":{"displayName":"Boris Culjak","userId":"11993007517939214783"}},"outputId":"9a5226f6-c0e3-4d6a-957a-f1bbfee48800"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/facebookresearch/pytorch3d.git@stable\n","  Cloning https://github.com/facebookresearch/pytorch3d.git (to revision stable) to /tmp/pip-req-build-ax06169l\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorch3d.git /tmp/pip-req-build-ax06169l\n","  Running command git checkout -q 75ebeeaea0908c5527e7b1e305fbc7681382db47\n","  Resolved https://github.com/facebookresearch/pytorch3d.git to commit 75ebeeaea0908c5527e7b1e305fbc7681382db47\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting iopath (from pytorch3d==0.7.8)\n","  Downloading iopath-0.1.10.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (4.67.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (4.12.2)\n","Collecting portalocker (from iopath->pytorch3d==0.7.8)\n","  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n","Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n","Building wheels for collected packages: pytorch3d, iopath\n","  Building wheel for pytorch3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorch3d: filename=pytorch3d-0.7.8-cp311-cp311-linux_x86_64.whl size=60242890 sha256=90afb9b421631509d0902b5c272dac9432263d2ba980b31e31b9c1071057eac8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-x5laa6ez/wheels/08/90/1b/df18c3e3634f86278e793b87f37ea4c58d0c36731196122518\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=66700f98cf12c56505acdecd11eadf1e70d6ae4b8101abb3a6101e763f2e1e91\n","  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n","Successfully built pytorch3d iopath\n","Installing collected packages: portalocker, iopath, pytorch3d\n","Successfully installed iopath-0.1.10 portalocker-3.1.1 pytorch3d-0.7.8\n"]}],"source":["import os\n","import sys\n","import torch\n","need_pytorch3d=False\n","try:\n","    import pytorch3d\n","except ModuleNotFoundError:\n","    need_pytorch3d=True\n","if need_pytorch3d:\n","    if torch.__version__.startswith(\"2.2.\") and sys.platform.startswith(\"linux\"):\n","        # We try to install PyTorch3D via a released wheel.\n","        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n","        version_str=\"\".join([\n","            f\"py3{sys.version_info.minor}_cu\",\n","            torch.version.cuda.replace(\".\",\"\"),\n","            f\"_pyt{pyt_version_str}\"\n","        ])\n","        !pip install fvcore iopath\n","        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n","    else:\n","        # We try to install PyTorch3D from source.\n","        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"]},{"cell_type":"code","source":["import torch\n","from pytorch3d.transforms.so3 import (\n","    so3_exp_map,\n","    so3_relative_angle,\n",")\n","from pytorch3d.renderer.cameras import (\n","    SfMPerspectiveCameras,\n",")\n","\n","# add path for demo utils\n","import sys\n","import os\n","sys.path.append(os.path.abspath(''))\n","\n","# set for reproducibility\n","torch.manual_seed(42)\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"WARNING: CPU only, this will be slow!\")"],"metadata":{"id":"iFxEVxCw67p3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/camera_visualization.py\n","from camera_visualization import plot_camera_scene\n","\n","!mkdir data\n","!wget -P data https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/data/camera_graph.pth"],"metadata":{"id":"8qmzfRaP7CZR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743091064294,"user_tz":-60,"elapsed":836,"user":{"displayName":"Boris Culjak","userId":"11993007517939214783"}},"outputId":"9149b845-4bb0-416d-edaf-6a1bbae76f0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-03-27 15:57:43--  https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/camera_visualization.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2037 (2.0K) [text/plain]\n","Saving to: ‘camera_visualization.py’\n","\n","\rcamera_visualizatio   0%[                    ]       0  --.-KB/s               \rcamera_visualizatio 100%[===================>]   1.99K  --.-KB/s    in 0s      \n","\n","2025-03-27 15:57:43 (24.1 MB/s) - ‘camera_visualization.py’ saved [2037/2037]\n","\n","--2025-03-27 15:57:43--  https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/data/camera_graph.pth\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16896 (16K) [application/octet-stream]\n","Saving to: ‘data/camera_graph.pth’\n","\n","camera_graph.pth    100%[===================>]  16.50K  --.-KB/s    in 0.001s  \n","\n","2025-03-27 15:57:43 (24.4 MB/s) - ‘data/camera_graph.pth’ saved [16896/16896]\n","\n"]}]},{"cell_type":"markdown","source":["# Camera setup and ground thruts"],"metadata":{"id":"d5XDKBtK7F3N"}},{"cell_type":"code","source":["camera_graph_file = './data/camera_graph.pth'\n","(R_absolute_gt, T_absolute_gt), (R_relative, T_relative), relative_edges = torch.load(camera_graph_file)"],"metadata":{"id":"Z0aNTm-V7HoT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- R stands for Rotation matrix. It is part of the camera's extrinsic parameters and describes how the camera is oriented in space. This is typically a 3x3 matrix.\n","- T stands for Translation vector. It also belongs to the camera's extrinsic parameters and tells you how the camera is positioned in space relative to some reference point. This is usually a 3-dimensional vector."],"metadata":{"id":"ah5RYtAe9QUR"}},{"cell_type":"markdown","source":["- Ground Truth (Absolute) Coordinates: These are the actual (real-world) positions and orientations of the cameras. These are known and fixed, used as a reference to assess the accuracy of estimated positions.\n","\n","- Relative Coordinates: These describe the position and orientation of one camera relative to another. For example, if you know the position of Camera A and you have the relative position of Camera B to Camera A, you can calculate the position of Camera B."],"metadata":{"id":"wrFTxXxGO0OJ"}},{"cell_type":"markdown","source":["relative_edges: A list of pairs of indices indicating between which cameras the relative positions are known."],"metadata":{"id":"8gr9yE3uPKlG"}},{"cell_type":"code","source":["cameras_relative = SfMPerspectiveCameras(\n","    R = R_relative.to(device),\n","    T = T_relative.to(device),\n","    device = device,\n",")"],"metadata":{"id":"ZH6cVGsZ9GvO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cameras_absolute_gt = SfMPerspectiveCameras(\n","    R = R_absolute_gt.to(device),\n","    T = T_absolute_gt.to(device),\n","    device = device,\n",")"],"metadata":{"id":"79f3MAxG94iA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The SfMPerspectiveCameras class in PyTorch3D is used to model cameras within a 3D environment by setting their positions and orientations through rotation matrices and translation vectors. This class is particularly useful in computer vision and 3D reconstruction tasks, such as Structure from Motion (SfM), where the goal is to reconstruct a scene's 3D structure from multiple 2D images. It allows for the simulation of camera behavior under different poses, and it's crucial for algorithms that need to estimate or optimize camera positions to align virtual camera views with actual observed data."],"metadata":{"id":"WZRgrHRbPyfg"}},{"cell_type":"code","source":["# the number of absolute camera positions\n","N = R_absolute_gt.shape[0]"],"metadata":{"id":"uMC2itKR96zG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Optimization Functions\n","\n","We now define two functions crucial for the optimization.\n","\n","calc_camera_distance compares a pair of cameras. This function is important as it defines the loss that we are minimizing. The method utilizes the so3_relative_angle function from the SO3 API.\n","\n","get_relative_camera computes the parameters of a relative camera that maps between a pair of absolute cameras. Here we utilize the compose and inverse class methods from the PyTorch3D Transforms API."],"metadata":{"id":"gOVuLAAB_H6y"}},{"cell_type":"code","source":["def calc_camera_distance(cam_1, cam_2):\n","    \"\"\"\n","    Calculates the divergence of a batch of pairs of cameras cam_1, cam_2.\n","    The distance is composed of the cosine of the relative angle between\n","    the rotation components of the camera extrinsics and the l2 distance\n","    between the translation vectors.\n","    \"\"\"\n","    # rotation distance\n","    R_distance = (1.-so3_relative_angle(cam_1.R, cam_2.R, cos_angle=True)).mean()\n","    # translation distance\n","    T_distance = ((cam_1.T - cam_2.T)**2).sum(1).mean()\n","    # the final distance is the sum\n","    return R_distance + T_distance"],"metadata":{"id":"ddpp5CR7Bhhz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The function calc_camera_distance(cam_1, cam_2) measures the difference between two cameras in terms of their orientation and position.\n","\n","The rotation distance evaluates how much the orientation of one camera differs from another. This is done by computing the cosine of the angle between their rotation matrices using the so3_relative_angle function, which outputs the cosine of the angle between these rotations. The computation 1 - cos(theta) translates this cosine value into a scale from 0 to 2, where 0 indicates that the cameras have identical orientations, and 2 suggests that they are oppositely directed (180 degrees apart).\n","\n","The translation distance is the straight-line distance between the positions of the two cameras. It's calculated as the Euclidean (L2) distance between their translation vectors, which involves summing the squares of the differences between corresponding components of these vectors and then taking the square root.\n","\n","After calculating these two distances, the function sums them to provide a single metric that quantifies the total difference in camera poses. This combined measure is particularly useful in scenarios like camera calibration or multi-camera system alignments where you need to match the orientation and position of one camera to another.\n","\n","This combination of rotation and translation measurements—one based on angles and the other on distance—doesn't automatically balance these different units. In practice, you might need to apply weights or scaling factors"],"metadata":{"id":"fKXy5wEdQV9I"}},{"cell_type":"code","source":["def get_relative_camera(cams, edges):\n","    \"\"\"\n","    For each pair of indices (i,j) in \"edges\" generate a camera\n","    that maps from the coordinates of the camera cams[i] to\n","    the coordinates of the camera cams[j]\n","    \"\"\"\n","\n","    # first generate the world-to-view Transform3d objects of each\n","    # camera pair (i, j) according to the edges argument\n","    trans_i, trans_j = [\n","        SfMPerspectiveCameras(\n","            R = cams.R[edges[:, i]],\n","            T = cams.T[edges[:, i]],\n","            device = device,\n","        ).get_world_to_view_transform()\n","         for i in (0, 1)\n","    ]\n","\n","    # compose the relative transformation as g_i^{-1} g_j\n","    trans_rel = trans_i.inverse().compose(trans_j)\n","\n","    # generate a camera from the relative transform\n","    matrix_rel = trans_rel.get_matrix()\n","    cams_relative = SfMPerspectiveCameras(\n","                        R = matrix_rel[:, :3, :3],\n","                        T = matrix_rel[:, 3, :3],\n","                        device = device,\n","                    )\n","    return cams_relative"],"metadata":{"id":"jvu0JOzAQKFi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The get_relative_camera(cams, edges) function is designed to compute the relative transformation between pairs of cameras based on given indices. It essentially tells you how to adjust from the perspective of one camera to match another within the same system.\n","\n","1. Extract Camera Transformations: For each camera pair specified by edges, the function first retrieves the transformation of each camera. This transformation describes how each camera views the world—essentially, how to translate and rotate world coordinates to fit the camera’s own coordinate system. This involves both a rotation (which way the camera is looking) and a translation (where the camera is positioned in space).\n","\n","2. Invert and Compose Transformations: For each pair, the function calculates what adjustments are needed to move from the first camera's viewpoint to the second's. This is done by first inverting the transformation of the first camera (to undo its perspective) and then applying the transformation of the second camera (to adopt its perspective). The inversion essentially resets the viewpoint to a neutral position, and applying the second transformation shifts this neutral viewpoint to that of the second camera.\n","\n","3. Create New Camera Models: From the combined transformations, the function then constructs a new camera model for each pair. These new models do not correspond to physical cameras but rather represent the relative orientations and positions between pairs of cameras as computed from their transformations."],"metadata":{"id":"xz3nVSVlSi3S"}},{"cell_type":"markdown","source":["The syntax for i in (0, 1) in the list comprehension is a Pythonic way to loop twice: first to process the transformation of the first camera in each pair (i=0) and then the second camera (i=1). This helps efficiently generate the transformations for both cameras in each pair using a concise code structure.\n","\n","In summary, the function does not convert relative coordinates to absolute coordinates. Instead, it provides a way to understand and model how each camera in a specified pair is positioned relative to the other. This relative modeling is fundamental in multi-camera setups where understanding the spatial relationships and orientations between cameras directly impacts the accuracy and effectiveness of the overall system's operation."],"metadata":{"id":"otpWbbkuUEG_"}},{"cell_type":"markdown","source":["# Optimization\n","\n","Finally, we start the optimization of the absolute cameras.\n","\n","We use SGD with momentum and optimize over log_R_absolute and T_absolute.\n","\n","As mentioned earlier, log_R_absolute is the axis angle representation of the rotation part of our absolute cameras. We can obtain the 3x3 rotation matrix R_absolute that corresponds to log_R_absolute with:\n","\n","R_absolute = so3_exp_map(log_R_absolute)\n"],"metadata":{"id":"kTRqlqUrBlKr"}},{"cell_type":"code","source":["# initialize the absolute log-rotations/translations with random entries\n","log_R_absolute_init = torch.randn(N, 3, dtype=torch.float32, device=device)\n","T_absolute_init = torch.randn(N, 3, dtype=torch.float32, device=device)\n","\n","# furthermore, we know that the first camera is a trivial one\n","#    (see the description above)\n","log_R_absolute_init[0, :] = 0.\n","T_absolute_init[0, :] = 0."],"metadata":{"id":"oKWLOsp01ecT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" These variables are initialized to store the initial guesses for the rotations and translations of a set of cameras in a scene. log_R_absolute_init holds the logarithmic representation of rotations, and T_absolute_init holds the translations. Each camera in the system is represented by three values in these tensors:\n"," - Rotation (log_R_absolute_init): The rotation of each camera is stored in a compact, logarithmic form (specifically, the axis-angle representation). This form expresses a rotation in 3D space as a vector along the rotation axis, with a magnitude equal to the angle of rotation in radians.\n"," - Translation (T_absolute_init): This simply stores the x, y, and z coordinates of each camera's position in space.\n","The use of torch.randn function initializes these values with random entries drawn from a normal distribution. This randomness serves as an initial guess for the positions and orientations of the cameras, which will be refined through an optimization process."],"metadata":{"id":"bX57yXGt1h0n"}},{"cell_type":"markdown","source":["The first camera is often set as a reference or anchor in the scene, hence its rotation and translation are initialized to zero"],"metadata":{"id":"6f1fLGtK18bB"}},{"cell_type":"markdown","source":["Using the logarithmic form for rotations, specifically the axis-angle representation, simplifies gradient calculations and allows for linear operations like interpolation and extrapolation. This approach avoids the constraints of orthogonality and unit norm required by rotation matrices and quaternions, making the optimization process more straightforward."],"metadata":{"id":"fCyesA2m2Gmb"}},{"cell_type":"code","source":["# instantiate a copy of the initialization of log_R / T\n","log_R_absolute = log_R_absolute_init.clone().detach()\n","log_R_absolute.requires_grad = True\n","T_absolute = T_absolute_init.clone().detach()\n","T_absolute.requires_grad = True"],"metadata":{"id":"y11Nc4ra8xt4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Cloning**: The .clone() method creates a copy of the original tensor (log_R_absolute_init and T_absolute_init). This is important because you often want to keep the initial values unchanged for reference or reuse them later without affecting them during the optimization process. Cloning ensures that the original tensors remain intact.\n","\n","**Detaching:** The .detach() method is used to detach the cloned tensors from the current computation graph. In PyTorch, tensors that are part of a computation graph record operations performed on them to compute gradients. By detaching the clones, you prevent the original computation history from being affected by the operations that will be performed on these new tensors. Essentially, this makes the new tensors independent of the original ones in terms of gradient computation.\n","\n","**requires_grad = True**: This setting is crucial for enabling automatic differentiation on these tensors. By setting requires_grad to True, PyTorch knows that it needs to compute gradients for these tensors when performing backpropagation. This is necessary because, in an optimization loop, you want to adjust these values (rotations and translations) to minimize the loss function, and gradients are required for the optimization algorithm (like SGD) to update the parameters."],"metadata":{"id":"XXcrDV8c8--K"}},{"cell_type":"code","source":["# the mask the specifies which cameras are going to be optimized\n","#     (since we know the first camera is already correct,\n","#      we only optimize over the 2nd-to-last cameras)\n","camera_mask = torch.ones(N, 1, dtype=torch.float32, device=device)\n","camera_mask[0] = 0."],"metadata":{"id":"9SLDl8Cg_PQz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The concept of a \"mask\" in this context is used to control which elements in a dataset or tensor are affected by certain operations, typically during computation processes like optimization. In your script, a mask is utilized to specify which cameras in a multi-camera system should undergo optimization.\n","\n","Creating the Mask: The mask is created as a tensor of ones (torch.ones(N, 1, dtype=torch.float32, device=device)), which initially suggests that all cameras are candidates for optimization. The size of the mask (N, 1) matches the number of cameras, and each entry in the mask corresponds to a camera.\n","\n","Setting the First Camera to Zero: By setting the first entry of the mask to zero (camera_mask[0] = 0), the script explicitly excludes the first camera from the optimization process. The value 0 indicates that any operations controlled by the mask should not affect this camera.\n","\n","First Camera as Reference: As mentioned previously, the first camera is often set to have zero rotation and zero translation, aligning it perfectly with the coordinate system's origin and orientation. This camera serves as a fixed reference or baseline for the system. Since its position and orientation are already defined as correct, there is no need to adjust or optimize its parameters.\n","\n","Optimization of Other Cameras: The remaining cameras (from the second to the last) are marked by the mask with a value of 1, indicating they are active for optimization. These cameras' parameters will be adjusted during the optimization process to align their observed data (like positions and rotations relative to the scene or other cameras) with the model or expected outcomes."],"metadata":{"id":"4QzbeGsz_Ra9"}},{"cell_type":"code","source":["# init the optimizer\n","optimizer = torch.optim.SGD([log_R_absolute, T_absolute], lr=.1, momentum=0.9)"],"metadata":{"id":"KOBJbstrAM-E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This initializes an optimizer using PyTorch's Stochastic Gradient Descent (SGD) method, which is designed to update the parameters (in this case, log_R_absolute and T_absolute) to minimize a loss function over iterations. The lr=.1 specifies the learning rate, which controls how much the parameters change in response to the calculated gradient during each update, and momentum=0.9 helps accelerate the optimizer in the right direction, thus improving the convergence. This setup does not define the criteria of optimality or loss function itself; it simply sets up the mechanism for updating parameters once the loss is computed during the optimization loop.\n"],"metadata":{"id":"nTNHUJJGANhq"}},{"cell_type":"code","source":["# run the optimization\n","n_iter = 2000  # fix the number of iterations\n","for it in range(n_iter):\n","    # re-init the optimizer gradients\n","    optimizer.zero_grad()\n","\n","    # compute the absolute camera rotations as\n","    # an exponential map of the logarithms (=axis-angles)\n","    # of the absolute rotations\n","    R_absolute = so3_exp_map(log_R_absolute * camera_mask)\n","\n","    # get the current absolute cameras\n","    cameras_absolute = SfMPerspectiveCameras(\n","        R = R_absolute,\n","        T = T_absolute * camera_mask,\n","        device = device,\n","    )\n","\n","    # compute the relative cameras as a composition of the absolute cameras\n","    cameras_relative_composed = \\\n","        get_relative_camera(cameras_absolute, relative_edges)\n","\n","    # compare the composed cameras with the ground truth relative cameras\n","    # camera_distance corresponds to $d$ from the description\n","    camera_distance = \\\n","        calc_camera_distance(cameras_relative_composed, cameras_relative)\n","\n","    # our loss function is the camera_distance\n","    camera_distance.backward()\n","\n","    # apply the gradients\n","    optimizer.step()\n","\n","    # plot and print status message\n","    if it % 200==0 or it==n_iter-1:\n","        status = 'iteration=%3d; camera_distance=%1.3e' % (it, camera_distance)\n","        plot_camera_scene(cameras_absolute, cameras_absolute_gt, status)\n","\n","print('Optimization finished.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"XRqQ_znYBjHH","executionInfo":{"status":"error","timestamp":1743152841569,"user_tz":-60,"elapsed":202,"user":{"displayName":"Boris Culjak","userId":"11993007517939214783"}},"outputId":"3c1e0f69-cf53-4a0f-d074-1b133289086a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'optimizer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d32eed02b3e3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# re-init the optimizer gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# compute the absolute camera rotations as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"]}]},{"cell_type":"markdown","source":["The function so3_exp_map in PyTorch is used to convert rotations from their logarithmic form (specifically the axis-angle representation) to the corresponding rotation matrices"],"metadata":{"id":"xB3ds2xyCGrW"}},{"cell_type":"markdown","source":["\n","\n","```\n","cameras_absolute = SfMPerspectiveCameras(\n","    R = R_absolute,\n","    T = T_absolute * camera_mask,\n","    device = device,\n",")\n","```\n","Initializing a set of cameras with their absolute rotations (R_absolute) and translations (T_absolute). The camera_mask applied to T_absolute ensures that the translation of the first camera remains zero (as it's set as the reference camera and does not need optimization). These cameras_absolute now represent the current estimate of where each camera is positioned and how it is oriented in your scene, based on the optimizer's current state.\n"],"metadata":{"id":"0nnGsCauDi48"}},{"cell_type":"markdown","source":["\n","\n","```\n","cameras_relative_composed = get_relative_camera(cameras_absolute, relative_edges)\n","```\n","\n","This line calculates the relative transformations between pairs of cameras specified in relative_edges. The function get_relative_camera uses the absolute transformations (positions and orientations) of these cameras (provided in cameras_absolute) to compute how one camera is positioned relative to another.\n","\n","For each pair of cameras (i, j) specified in relative_edges, this function:\n","\n","- Takes the absolute transformation of camera i, calculates its inverse (essentially setting it as a new reference point).\n","\n","- Applies the absolute transformation of camera j to this reference point, resulting in the transformation that describes how to move from camera i to camera j in space.\n"],"metadata":{"id":"GQXsXYYyHOdj"}},{"cell_type":"code","source":[],"metadata":{"id":"eEDtjE0cHe3k"},"execution_count":null,"outputs":[]}]}